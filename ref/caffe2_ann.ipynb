{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n",
      "WARNING:root:Debug message: No module named 'caffe2.python.caffe2_pybind11_state_hip'\n"
     ]
    }
   ],
   "source": [
    "from caffe2.python import workspace\n",
    "from caffe2.python import model_helper\n",
    "from caffe2.python import brew, core, cnn\n",
    "from caffe2.proto import caffe2_pb2\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('digit-recognizer/train.csv')\n",
    "df_test = pd.read_csv('digit-recognizer/test.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare data\n",
    "labels_numpy = df.label.values\n",
    "features_numpy = df.loc[:, df.columns != 'label'].values / 255.0\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    features_numpy, labels_numpy, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test = df_test.values / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(db_name, images, labels=None):\n",
    "    # Create empty leveldb database\n",
    "    # TODO why can not create leveldb\n",
    "    db = core.C.create_db('minidb', db_name, core.C.Mode.new)\n",
    "    transaction = db.new_transaction()\n",
    "    \n",
    "    # Move all data to the database\n",
    "    for i in range(images.shape[0]):\n",
    "        tensor_protos = caffe2_pb2.TensorProtos()\n",
    "        \n",
    "        # Copy image with MNIST number\n",
    "        img_tensor = tensor_protos.protos.add()\n",
    "        img_tensor.dims.extend(images[i].shape)\n",
    "        img_tensor.data_type = 1\n",
    "        flatten_img = images[i].reshape(np.prod(images[i].shape))\n",
    "        img_tensor.float_data.extend(flatten_img)\n",
    "\n",
    "        # Copy label for each number\n",
    "        label_tensor = tensor_protos.protos.add()\n",
    "        label_tensor.data_type = 2\n",
    "        if labels is not None:\n",
    "            label_tensor.int32_data.append(labels[i])\n",
    "        else:\n",
    "            label_tensor.int32_data.append(-1)\n",
    "\n",
    "        # Add data in transaction\n",
    "        transaction.put('%0.6d' % i, tensor_protos.SerializeToString())\n",
    "\n",
    "    # Close the transaction and close the database\n",
    "    del transaction\n",
    "    del db\n",
    "\n",
    "create_database('/tmp/db_train', X_train, y_train)\n",
    "create_database('/tmp/db_validation', X_valid, y_valid)\n",
    "create_database('/tmp/db_test', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_input(model, blobs_out, batch_size, db, db_type):\n",
    "    dbreader_name = \"dbreader_\" + db\n",
    "    dbreader = model.param_init_net.CreateDB(\n",
    "        [],\n",
    "        dbreader_name,\n",
    "        db=db,\n",
    "        db_type=db_type,\n",
    "    )\n",
    "    return model.net.TensorProtosDBInput(\n",
    "        dbreader, blobs_out, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(name, db_name, batch_size=100, hidden_dim=150, output_dim=10, train=True, accuracy=True):\n",
    "    model = model_helper.ModelHelper(name=name)\n",
    "\n",
    "    # Prepare data input operator that will fetch data from DB\n",
    "    data, label = db_input(\n",
    "        model,\n",
    "        ['data', 'label'],\n",
    "        batch_size=batch_size,\n",
    "        db=db_name,\n",
    "        # db_type='leveldb')\n",
    "        db_type='minidb')\n",
    "    data = model.StopGradient(data, data)\n",
    "    fc1 = brew.fc(model, data, \"fc1\", dim_in=28 * 28, dim_out=hidden_dim)\n",
    "    # model.param_init_net.UniformFill([], \"fc1_b\", shape=[hidden_dim], min=-0.04, max=0.04)\n",
    "    relu1 = model.Relu(fc1, \"relu1\")\n",
    "    \n",
    "    fc2 = brew.fc(model, relu1, \"fc2\", dim_in=hidden_dim, dim_out=hidden_dim)\n",
    "    # model.param_init_net.UniformFill([], \"fc2_b\", shape=[hidden_dim], min=-0.08, max=0.08)\n",
    "    tanh2 = model.Tanh(fc2, \"tanh2\")\n",
    "    \n",
    "    fc3 = brew.fc(model, tanh2, \"fc3\", dim_in=hidden_dim, dim_out=hidden_dim)\n",
    "    # model.param_init_net.UniformFill([], \"fc3_b\", shape=[hidden_dim], min=-0.08, max=0.08)\n",
    "    elu3 = model.Elu(\"fc3\", \"elu3\")\n",
    "    \n",
    "    fc4 = brew.fc(model, \"elu3\", \"fc4\", dim_in=hidden_dim, dim_out=output_dim)\n",
    "    # model.param_init_net.UniformFill([], \"fc4_b\", shape=[output_dim], min=-0.08, max=0.08)\n",
    "    \n",
    "    softmax = model.Softmax(fc4, 'softmax')\n",
    "\n",
    "    # Check if we need to add training operators\n",
    "    if train:\n",
    "        # Prepare Cross Entropy operators with loss\n",
    "        xent = model.LabelCrossEntropy([softmax, label], 'xent')\n",
    "        loss = model.AveragedLoss(xent, \"loss\")\n",
    "\n",
    "        # Add all gradient operators that will be needed to calculate our loss and train our model\n",
    "        model.AddGradientOperators([loss])\n",
    "        \n",
    "        # Prepare variables for SGD\n",
    "        ITER = model.Iter([], \"iter\")\n",
    "        # LR = model.LearningRate(ITER, \"LR\", base_lr=-0.1, policy=\"step\", stepsize=1, gamma=0.999)\n",
    "        lr = -0.02\n",
    "        # lr = -0.04\n",
    "        LR = model.param_init_net.ConstantFill([], \"LR\", shape=[1], value=lr)\n",
    "        ONE = model.param_init_net.ConstantFill([], \"ONE\", shape=[1], value=1.0)\n",
    "        \n",
    "        # Update all gradients for each params\n",
    "        for param in model.params:\n",
    "            # Note how we get the gradient of each parameter - CNNModelHelper keeps\n",
    "            # track of that\n",
    "            param_grad = model.param_to_grad[param]\n",
    "            \n",
    "            # The update is a simple weighted sum: param = param + param_grad * LR\n",
    "            model.WeightedSum([param, ONE, param_grad, LR], param)\n",
    "\n",
    "    # Add accuracy metrics if needed\n",
    "    if accuracy:\n",
    "        model.Accuracy([softmax, label], \"accuracy\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Relu.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Elu.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Iter.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Relu.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Elu.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Relu.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Elu.\n"
     ]
    }
   ],
   "source": [
    "train_model = create_model(\"train\", \"/tmp/db_train\")\n",
    "validation_model = create_model(\"validation\", \"/tmp/db_validation\", train=False)\n",
    "test_model = create_model('test_model', '/tmp/db_test', train=False, accuracy=False)\n",
    "\n",
    "workspace.RunNetOnce(test_model.param_init_net)\n",
    "workspace.RunNetOnce(validation_model.param_init_net)\n",
    "workspace.RunNetOnce(train_model.param_init_net)\n",
    "\n",
    "with open('/tmp/proto', 'w') as f:\n",
    "    f.write(str(train_model.net.Proto()))\n",
    "with open('/tmp/proto.init', 'w') as f:\n",
    "    f.write(str(train_model.param_init_net.Proto()))\n",
    "    \n",
    "with open('/tmp/proto.init.val', 'w') as f:\n",
    "    f.write(str(validation_model.param_init_net.Proto()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_validation_accuracy():\n",
    "    # Initialize our model\n",
    "    # workspace.RunNetOnce(validation_model.param_init_net)\n",
    "    workspace.CreateNet(validation_model.net, overwrite=True)\n",
    "    \n",
    "    # Iterate over all validation dataset\n",
    "    all_accuracy = []\n",
    "    for i in range(X_valid.shape[0]//100):\n",
    "        workspace.RunNet(validation_model.net.Proto().name)\n",
    "        all_accuracy.append(workspace.FetchBlob('accuracy'))\n",
    "    \n",
    "    # Return mean accuracy for validation dataset\n",
    "    return np.array(all_accuracy).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect():\n",
    "    fc1_b = workspace.FetchBlob(\"fc1_b\")\n",
    "    print(fc1_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #500/10000 TIME_per_epoch: 0.120s TRAIN_Loss: 0.3850 TRAIN_Acc: 0.9100 VAL_Acc: 0.8923\n",
      "Epoch #1000/10000 TIME_per_epoch: 0.099s TRAIN_Loss: 0.2861 TRAIN_Acc: 0.8800 VAL_Acc: 0.9143\n",
      "Epoch #1500/10000 TIME_per_epoch: 0.101s TRAIN_Loss: 0.1244 TRAIN_Acc: 0.9800 VAL_Acc: 0.9239\n",
      "Epoch #2000/10000 TIME_per_epoch: 0.097s TRAIN_Loss: 0.1886 TRAIN_Acc: 0.9600 VAL_Acc: 0.9307\n",
      "Epoch #2500/10000 TIME_per_epoch: 0.098s TRAIN_Loss: 0.2135 TRAIN_Acc: 0.9400 VAL_Acc: 0.9393\n",
      "Epoch #3000/10000 TIME_per_epoch: 0.097s TRAIN_Loss: 0.0904 TRAIN_Acc: 0.9700 VAL_Acc: 0.9445\n",
      "Epoch #3500/10000 TIME_per_epoch: 0.145s TRAIN_Loss: 0.1791 TRAIN_Acc: 0.9400 VAL_Acc: 0.9486\n",
      "Epoch #4000/10000 TIME_per_epoch: 0.096s TRAIN_Loss: 0.0442 TRAIN_Acc: 0.9900 VAL_Acc: 0.9537\n",
      "Epoch #4500/10000 TIME_per_epoch: 0.097s TRAIN_Loss: 0.2320 TRAIN_Acc: 0.9100 VAL_Acc: 0.9540\n",
      "Epoch #5000/10000 TIME_per_epoch: 0.097s TRAIN_Loss: 0.0880 TRAIN_Acc: 0.9700 VAL_Acc: 0.9565\n",
      "Epoch #5500/10000 TIME_per_epoch: 0.105s TRAIN_Loss: 0.1856 TRAIN_Acc: 0.9600 VAL_Acc: 0.9569\n",
      "Epoch #6000/10000 TIME_per_epoch: 0.096s TRAIN_Loss: 0.1302 TRAIN_Acc: 0.9800 VAL_Acc: 0.9570\n",
      "Epoch #6500/10000 TIME_per_epoch: 0.095s TRAIN_Loss: 0.0882 TRAIN_Acc: 0.9900 VAL_Acc: 0.9604\n",
      "Epoch #7000/10000 TIME_per_epoch: 0.096s TRAIN_Loss: 0.0632 TRAIN_Acc: 0.9800 VAL_Acc: 0.9630\n",
      "Epoch #7500/10000 TIME_per_epoch: 0.102s TRAIN_Loss: 0.0739 TRAIN_Acc: 0.9800 VAL_Acc: 0.9636\n",
      "Epoch #8000/10000 TIME_per_epoch: 0.101s TRAIN_Loss: 0.1870 TRAIN_Acc: 0.9900 VAL_Acc: 0.9638\n",
      "Epoch #8500/10000 TIME_per_epoch: 0.096s TRAIN_Loss: 0.0358 TRAIN_Acc: 0.9900 VAL_Acc: 0.9649\n",
      "Epoch #9000/10000 TIME_per_epoch: 0.098s TRAIN_Loss: 0.0335 TRAIN_Acc: 1.0000 VAL_Acc: 0.9649\n",
      "Epoch #9500/10000 TIME_per_epoch: 0.097s TRAIN_Loss: 0.0096 TRAIN_Acc: 1.0000 VAL_Acc: 0.9665\n",
      "Epoch #10000/10000 TIME_per_epoch: 0.097s TRAIN_Loss: 0.0915 TRAIN_Acc: 0.9800 VAL_Acc: 0.9667\n"
     ]
    }
   ],
   "source": [
    "# Initialize out training model\n",
    "workspace.RunNetOnce(train_model.param_init_net)\n",
    "# inspect()\n",
    "workspace.CreateNet(train_model.net, overwrite=True)\n",
    "\n",
    "# Iterate over all epochs\n",
    "# NUMBER_OF_EPOCHS = 10000\n",
    "NUMBER_OF_EPOCHS = 10000\n",
    "for i in range(NUMBER_OF_EPOCHS):\n",
    "    # Train our model\n",
    "    start_time = time.time()\n",
    "    workspace.RunNet(train_model.net.Proto().name)\n",
    "    \n",
    "    # Once per 20 epochs let's run validation and print results\n",
    "    if (i+1) % 500 == 0:\n",
    "        train_loss = workspace.FetchBlob('loss')\n",
    "        train_accuracy = workspace.FetchBlob('accuracy')\n",
    "        val_accuracy = calculate_validation_accuracy()\n",
    "        epoch_time = time.time()-start_time\n",
    "        print(('Epoch #%d/%d TIME_per_epoch: %.3fs '+\n",
    "               'TRAIN_Loss: %.4f TRAIN_Acc: %.4f '+\n",
    "               'VAL_Acc: %.4f') % (i+1, NUMBER_OF_EPOCHS, epoch_time, train_loss, train_accuracy, val_accuracy))\n",
    "        # inspect()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting #20/280.0...\n",
      "Predicting #40/280.0...\n",
      "Predicting #60/280.0...\n",
      "Predicting #80/280.0...\n",
      "Predicting #100/280.0...\n",
      "Predicting #120/280.0...\n",
      "Predicting #140/280.0...\n",
      "Predicting #160/280.0...\n",
      "Predicting #180/280.0...\n",
      "Predicting #200/280.0...\n",
      "Predicting #220/280.0...\n",
      "Predicting #240/280.0...\n",
      "Predicting #260/280.0...\n",
      "Predicting #280/280.0...\n"
     ]
    }
   ],
   "source": [
    "# Initialize out prediction model\n",
    "\n",
    "# workspace.RunNetOnce(test_model.param_init_net)\n",
    "workspace.CreateNet(test_model.net, overwrite=True)\n",
    "\n",
    "# Iterate over all test dataset\n",
    "predicted_labels = []\n",
    "for i in range(X_test.shape[0]//100):\n",
    "    # Run our model for predicting labels\n",
    "    workspace.RunNet(test_model.net.Proto().name)\n",
    "    batch_prediction = workspace.FetchBlob('softmax')\n",
    "    if (i+1) % 20 == 0:\n",
    "        print('Predicting #{}/{}...'.format(i+1, X_test.shape[0]/100))\n",
    "    \n",
    "    # Retrieve labels\n",
    "    for prediction in batch_prediction:\n",
    "        predicted_labels.append(np.argmax(prediction))  # Label = index of max argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dict = {}\n",
    "for i, v in enumerate(predicted_labels):\n",
    "    raw_dict[i + 1] = v\n",
    "\n",
    "out_df = pd.DataFrame(\n",
    "    data={\n",
    "        \"ImageId\": raw_dict.keys(),\n",
    "        \"Label\": raw_dict.values(),\n",
    "    }\n",
    ")\n",
    "out_df.to_csv('/tmp/caffe2_nn_2', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
