{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n",
      "WARNING:root:Debug message: No module named 'caffe2.python.caffe2_pybind11_state_hip'\n"
     ]
    }
   ],
   "source": [
    "from caffe2.python import workspace\n",
    "from caffe2.python import model_helper\n",
    "from caffe2.python import brew, core, cnn\n",
    "from caffe2.proto import caffe2_pb2\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('digit-recognizer/train.csv')\n",
    "df_test = pd.read_csv('digit-recognizer/test.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare data\n",
    "labels_numpy = df.label.values\n",
    "features_numpy = df.loc[:, df.columns != 'label'].values / 255.0\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    features_numpy, labels_numpy, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test = df_test.values / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(db_name, images, labels=None):\n",
    "    # Create empty leveldb database\n",
    "    # TODO why can not create leveldb\n",
    "    db = core.C.create_db('minidb', db_name, core.C.Mode.write)\n",
    "    transaction = db.new_transaction()\n",
    "    \n",
    "    # Move all data to the database\n",
    "    for i in range(images.shape[0]):\n",
    "        tensor_protos = caffe2_pb2.TensorProtos()\n",
    "        \n",
    "        # Copy image with MNIST number\n",
    "        img_tensor = tensor_protos.protos.add()\n",
    "        img_tensor.dims.extend(images[i].shape)\n",
    "        img_tensor.data_type = 1\n",
    "        flatten_img = images[i].reshape(np.prod(images[i].shape))\n",
    "        img_tensor.float_data.extend(flatten_img)\n",
    "\n",
    "        # Copy label for each number\n",
    "        label_tensor = tensor_protos.protos.add()\n",
    "        label_tensor.data_type = 2\n",
    "        if labels is not None:\n",
    "            label_tensor.int32_data.append(labels[i])\n",
    "        else:\n",
    "            label_tensor.int32_data.append(-1)\n",
    "\n",
    "        # Add data in transaction\n",
    "        transaction.put('%0.6d' % i, tensor_protos.SerializeToString())\n",
    "\n",
    "    # Close the transaction and close the database\n",
    "    del transaction\n",
    "    del db\n",
    "\n",
    "create_database('db_train', X_train, y_train)\n",
    "create_database('db_validation', X_valid, y_valid)\n",
    "create_database('db_test', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_input(model, blobs_out, batch_size, db, db_type):\n",
    "    dbreader_name = \"dbreader_\" + db\n",
    "    dbreader = model.param_init_net.CreateDB(\n",
    "        [],\n",
    "        dbreader_name,\n",
    "        db=db,\n",
    "        db_type=db_type,\n",
    "    )\n",
    "    return model.net.TensorProtosDBInput(\n",
    "        dbreader, blobs_out, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(name, db_name, batch_size=100, hidden_dim=150, output_dim=10, train=True, accuracy=True):\n",
    "    model = model_helper.ModelHelper(name=name)\n",
    "\n",
    "    # Prepare data input operator that will fetch data from DB\n",
    "    data, label = db_input(\n",
    "        model,\n",
    "        ['data', 'label'],\n",
    "        batch_size=batch_size,\n",
    "        db=db_name,\n",
    "        # db_type='leveldb')\n",
    "        db_type='minidb')\n",
    "    data = model.StopGradient(data, data)\n",
    "    fc1 = brew.fc(model, data, \"fc1\", dim_in=28 * 28, dim_out=hidden_dim)\n",
    "    relu1 = model.Relu(fc1, \"relu1\")\n",
    "    \n",
    "    fc2 = brew.fc(model, relu1, \"fc2\", dim_in=hidden_dim, dim_out=hidden_dim)\n",
    "    tanh2 = model.Tanh(fc2, \"tanh2\")\n",
    "    \n",
    "    fc3 = brew.fc(model, tanh2, \"fc3\", dim_in=hidden_dim, dim_out=hidden_dim)\n",
    "    elu3 = model.Elu(\"fc3\", \"elu3\")\n",
    "    \n",
    "    fc4 = brew.fc(model, \"elu3\", \"fc4\", dim_in=hidden_dim, dim_out=output_dim)\n",
    "    \n",
    "    softmax = model.Softmax(fc4, 'softmax')\n",
    "\n",
    "    # Check if we need to add training operators\n",
    "    if train:\n",
    "        # Prepare Cross Entropy operators with loss\n",
    "        xent = model.LabelCrossEntropy([softmax, label], 'xent')\n",
    "        loss = model.AveragedLoss(xent, \"loss\")\n",
    "\n",
    "        # Add all gradient operators that will be needed to calculate our loss and train our model\n",
    "        model.AddGradientOperators([loss])\n",
    "        \n",
    "        # Prepare variables for SGD\n",
    "        ITER = model.Iter([], \"iter\")\n",
    "        # LR = model.LearningRate(ITER, \"LR\", base_lr=-0.1, policy=\"step\", stepsize=1, gamma=0.999)\n",
    "        LR = model.param_init_net.ConstantFill([], \"LR\", shape=[1], value=-0.02)\n",
    "        ONE = model.param_init_net.ConstantFill([], \"ONE\", shape=[1], value=1.0)\n",
    "        \n",
    "        # Update all gradients for each params\n",
    "        for param in model.params:\n",
    "            # Note how we get the gradient of each parameter - CNNModelHelper keeps\n",
    "            # track of that\n",
    "            param_grad = model.param_to_grad[param]\n",
    "            \n",
    "            # The update is a simple weighted sum: param = param + param_grad * LR\n",
    "            model.WeightedSum([param, ONE, param_grad, LR], param)\n",
    "\n",
    "    # Add accuracy metrics if needed\n",
    "    if accuracy:\n",
    "        model.Accuracy([softmax, label], \"accuracy\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Relu.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Elu.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Iter.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Relu.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Elu.\n"
     ]
    }
   ],
   "source": [
    "train_model = create_model(\"train\", \"db_train\")\n",
    "validation_model = create_model(\"validation\", \"db_validation\", train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"train\"\n",
       "op {\n",
       "  input: \"dbreader_db_train\"\n",
       "  output: \"data\"\n",
       "  output: \"label\"\n",
       "  name: \"\"\n",
       "  type: \"TensorProtosDBInput\"\n",
       "  arg {\n",
       "    name: \"batch_size\"\n",
       "    i: 100\n",
       "  }\n",
       "}\n",
       "op {\n",
       "  input: \"data\"\n",
       "  output: \"data\"\n",
       "  name: \"\"\n",
       "  type: \"StopGradient\"\n",
       "}\n",
       "op {\n",
       "  input: \"data\"\n",
       "  input: \"fc1_w\"\n",
       "  input: \"fc1_b\"\n",
       "  output: \"fc1\"\n",
       "  name: \"\"\n",
       "  type: \"FC\"\n",
       "  arg {\n",
       "    name: \"order\"\n",
       "    s: \"NCHW\"\n",
       "  }\n",
       "  arg {\n",
       "    name: \"use_cudnn\"\n",
       "    i: 1\n",
       "  }\n",
       "  arg {\n",
       "    name: \"cudnn_exhaustive_search\"\n",
       "    i: 0\n",
       "  }\n",
       "}\n",
       "op {\n",
       "  input: \"fc1\"\n",
       "  output: \"relu1\"\n",
       "  name: \"\"\n",
       "  type: \"Relu\"\n",
       "}\n",
       "op {\n",
       "  input: \"relu1\"\n",
       "  input: \"fc2_w\"\n",
       "  input: \"fc2_b\"\n",
       "  output: \"fc2\"\n",
       "  name: \"\"\n",
       "  type: \"FC\"\n",
       "  arg {\n",
       "    name: \"order\"\n",
       "    s: \"NCHW\"\n",
       "  }\n",
       "  arg {\n",
       "    name: \"use_cudnn\"\n",
       "    i: 1\n",
       "  }\n",
       "  arg {\n",
       "    name: \"cudnn_exhaustive_search\"\n",
       "    i: 0\n",
       "  }\n",
       "}\n",
       "op {\n",
       "  input: \"fc2\"\n",
       "  output: \"tanh2\"\n",
       "  name: \"\"\n",
       "  type: \"Tanh\"\n",
       "}\n",
       "op {\n",
       "  input: \"tanh2\"\n",
       "  input: \"fc3_w\"\n",
       "  input: \"fc3_b\"\n",
       "  output: \"fc3\"\n",
       "  name: \"\"\n",
       "  type: \"FC\"\n",
       "  arg {\n",
       "    name: \"order\"\n",
       "    s: \"NCHW\"\n",
       "  }\n",
       "  arg {\n",
       "    name: \"use_cudnn\"\n",
       "    i: 1\n",
       "  }\n",
       "  arg {\n",
       "    name: \"cudnn_exhaustive_search\"\n",
       "    i: 0\n",
       "  }\n",
       "}\n",
       "op {\n",
       "  input: \"fc3\"\n",
       "  output: \"elu3\"\n",
       "  name: \"\"\n",
       "  type: \"Elu\"\n",
       "}\n",
       "op {\n",
       "  input: \"elu3\"\n",
       "  input: \"fc4_w\"\n",
       "  input: \"fc4_b\"\n",
       "  output: \"fc4\"\n",
       "  name: \"\"\n",
       "  type: \"FC\"\n",
       "  arg {\n",
       "    name: \"order\"\n",
       "    s: \"NCHW\"\n",
       "  }\n",
       "  arg {\n",
       "    name: \"use_cudnn\"\n",
       "    i: 1\n",
       "  }\n",
       "  arg {\n",
       "    name: \"cudnn_exhaustive_search\"\n",
       "    i: 0\n",
       "  }\n",
       "}\n",
       "op {\n",
       "  input: \"fc4\"\n",
       "  output: \"softmax\"\n",
       "  name: \"\"\n",
       "  type: \"Softmax\"\n",
       "}\n",
       "op {\n",
       "  input: \"softmax\"\n",
       "  input: \"label\"\n",
       "  output: \"xent\"\n",
       "  name: \"\"\n",
       "  type: \"LabelCrossEntropy\"\n",
       "}\n",
       "op {\n",
       "  input: \"xent\"\n",
       "  output: \"loss\"\n",
       "  name: \"\"\n",
       "  type: \"AveragedLoss\"\n",
       "}\n",
       "op {\n",
       "  input: \"loss\"\n",
       "  output: \"loss_autogen_grad\"\n",
       "  name: \"\"\n",
       "  type: \"ConstantFill\"\n",
       "  arg {\n",
       "    name: \"value\"\n",
       "    f: 1.0\n",
       "  }\n",
       "}\n",
       "op {\n",
       "  input: \"xent\"\n",
       "  input: \"loss_autogen_grad\"\n",
       "  output: \"xent_grad\"\n",
       "  name: \"\"\n",
       "  type: \"AveragedLossGradient\"\n",
       "  is_gradient_op: true\n",
       "}\n",
       "op {\n",
       "  input: \"softmax\"\n",
       "  input: \"label\"\n",
       "  input: \"xent_grad\"\n",
       "  output: \"softmax_grad\"\n",
       "  name: \"\"\n",
       "  type: \"LabelCrossEntropyGradient\"\n",
       "  is_gradient_op: true\n",
       "}\n",
       "op {\n",
       "  input: \"softmax\"\n",
       "  input: \"softmax_grad\"\n",
       "  output: \"fc4_grad\"\n",
       "  name: \"\"\n",
       "  type: \"SoftmaxGradient\"\n",
       "  is_gradient_op: true\n",
       "}\n",
       "op {\n",
       "  input: \"elu3\"\n",
       "  input: \"fc4_w\"\n",
       "  input: \"fc4_grad\"\n",
       "  output: \"fc4_w_grad\"\n",
       "  output: \"fc4_b_grad\"\n",
       "  output: \"elu3_grad\"\n",
       "  name: \"\"\n",
       "  type: \"FCGradient\"\n",
       "  arg {\n",
       "    name: \"order\"\n",
       "    s: \"NCHW\"\n",
       "  }\n",
       "  arg {\n",
       "    name: \"use_cudnn\"\n",
       "    i: 1\n",
       "  }\n",
       "  arg {\n",
       "    name: \"cudnn_exhaustive_search\"\n",
       "    i: 0\n",
       "  }\n",
       "  is_gradient_op: true\n",
       "}\n",
       "op {\n",
       "  input: \"elu3\"\n",
       "  input: \"elu3_grad\"\n",
       "  output: \"fc3_grad\"\n",
       "  name: \"\"\n",
       "  type: \"EluGradient\"\n",
       "  is_gradient_op: true\n",
       "}\n",
       "op {\n",
       "  input: \"tanh2\"\n",
       "  input: \"fc3_w\"\n",
       "  input: \"fc3_grad\"\n",
       "  output: \"fc3_w_grad\"\n",
       "  output: \"fc3_b_grad\"\n",
       "  output: \"tanh2_grad\"\n",
       "  name: \"\"\n",
       "  type: \"FCGradient\"\n",
       "  arg {\n",
       "    name: \"order\"\n",
       "    s: \"NCHW\"\n",
       "  }\n",
       "  arg {\n",
       "    name: \"use_cudnn\"\n",
       "    i: 1\n",
       "  }\n",
       "  arg {\n",
       "    name: \"cudnn_exhaustive_search\"\n",
       "    i: 0\n",
       "  }\n",
       "  is_gradient_op: true\n",
       "}\n",
       "op {\n",
       "  input: \"tanh2\"\n",
       "  input: \"tanh2_grad\"\n",
       "  output: \"fc2_grad\"\n",
       "  name: \"\"\n",
       "  type: \"TanhGradient\"\n",
       "  is_gradient_op: true\n",
       "}\n",
       "op {\n",
       "  input: \"relu1\"\n",
       "  input: \"fc2_w\"\n",
       "  input: \"fc2_grad\"\n",
       "  output: \"fc2_w_grad\"\n",
       "  output: \"fc2_b_grad\"\n",
       "  output: \"relu1_grad\"\n",
       "  name: \"\"\n",
       "  type: \"FCGradient\"\n",
       "  arg {\n",
       "    name: \"order\"\n",
       "    s: \"NCHW\"\n",
       "  }\n",
       "  arg {\n",
       "    name: \"use_cudnn\"\n",
       "    i: 1\n",
       "  }\n",
       "  arg {\n",
       "    name: \"cudnn_exhaustive_search\"\n",
       "    i: 0\n",
       "  }\n",
       "  is_gradient_op: true\n",
       "}\n",
       "op {\n",
       "  input: \"relu1\"\n",
       "  input: \"relu1_grad\"\n",
       "  output: \"fc1_grad\"\n",
       "  name: \"\"\n",
       "  type: \"ReluGradient\"\n",
       "  is_gradient_op: true\n",
       "}\n",
       "op {\n",
       "  input: \"data\"\n",
       "  input: \"fc1_w\"\n",
       "  input: \"fc1_grad\"\n",
       "  output: \"fc1_w_grad\"\n",
       "  output: \"fc1_b_grad\"\n",
       "  output: \"data_grad\"\n",
       "  name: \"\"\n",
       "  type: \"FCGradient\"\n",
       "  arg {\n",
       "    name: \"order\"\n",
       "    s: \"NCHW\"\n",
       "  }\n",
       "  arg {\n",
       "    name: \"use_cudnn\"\n",
       "    i: 1\n",
       "  }\n",
       "  arg {\n",
       "    name: \"cudnn_exhaustive_search\"\n",
       "    i: 0\n",
       "  }\n",
       "  is_gradient_op: true\n",
       "}\n",
       "op {\n",
       "  output: \"iter\"\n",
       "  name: \"\"\n",
       "  type: \"Iter\"\n",
       "}\n",
       "op {\n",
       "  input: \"fc1_w\"\n",
       "  input: \"ONE\"\n",
       "  input: \"fc1_w_grad\"\n",
       "  input: \"LR\"\n",
       "  output: \"fc1_w\"\n",
       "  name: \"\"\n",
       "  type: \"WeightedSum\"\n",
       "}\n",
       "op {\n",
       "  input: \"fc1_b\"\n",
       "  input: \"ONE\"\n",
       "  input: \"fc1_b_grad\"\n",
       "  input: \"LR\"\n",
       "  output: \"fc1_b\"\n",
       "  name: \"\"\n",
       "  type: \"WeightedSum\"\n",
       "}\n",
       "op {\n",
       "  input: \"fc2_w\"\n",
       "  input: \"ONE\"\n",
       "  input: \"fc2_w_grad\"\n",
       "  input: \"LR\"\n",
       "  output: \"fc2_w\"\n",
       "  name: \"\"\n",
       "  type: \"WeightedSum\"\n",
       "}\n",
       "op {\n",
       "  input: \"fc2_b\"\n",
       "  input: \"ONE\"\n",
       "  input: \"fc2_b_grad\"\n",
       "  input: \"LR\"\n",
       "  output: \"fc2_b\"\n",
       "  name: \"\"\n",
       "  type: \"WeightedSum\"\n",
       "}\n",
       "op {\n",
       "  input: \"fc3_w\"\n",
       "  input: \"ONE\"\n",
       "  input: \"fc3_w_grad\"\n",
       "  input: \"LR\"\n",
       "  output: \"fc3_w\"\n",
       "  name: \"\"\n",
       "  type: \"WeightedSum\"\n",
       "}\n",
       "op {\n",
       "  input: \"fc3_b\"\n",
       "  input: \"ONE\"\n",
       "  input: \"fc3_b_grad\"\n",
       "  input: \"LR\"\n",
       "  output: \"fc3_b\"\n",
       "  name: \"\"\n",
       "  type: \"WeightedSum\"\n",
       "}\n",
       "op {\n",
       "  input: \"fc4_w\"\n",
       "  input: \"ONE\"\n",
       "  input: \"fc4_w_grad\"\n",
       "  input: \"LR\"\n",
       "  output: \"fc4_w\"\n",
       "  name: \"\"\n",
       "  type: \"WeightedSum\"\n",
       "}\n",
       "op {\n",
       "  input: \"fc4_b\"\n",
       "  input: \"ONE\"\n",
       "  input: \"fc4_b_grad\"\n",
       "  input: \"LR\"\n",
       "  output: \"fc4_b\"\n",
       "  name: \"\"\n",
       "  type: \"WeightedSum\"\n",
       "}\n",
       "op {\n",
       "  input: \"softmax\"\n",
       "  input: \"label\"\n",
       "  output: \"accuracy\"\n",
       "  name: \"\"\n",
       "  type: \"Accuracy\"\n",
       "}\n",
       "external_input: \"dbreader_db_train\"\n",
       "external_input: \"fc1_w\"\n",
       "external_input: \"fc1_b\"\n",
       "external_input: \"fc2_w\"\n",
       "external_input: \"fc2_b\"\n",
       "external_input: \"fc3_w\"\n",
       "external_input: \"fc3_b\"\n",
       "external_input: \"fc4_w\"\n",
       "external_input: \"fc4_b\"\n",
       "external_input: \"ONE\"\n",
       "external_input: \"LR\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.net.Proto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_validation_accuracy():\n",
    "    # Initialize our model\n",
    "    workspace.RunNetOnce(validation_model.param_init_net)\n",
    "    workspace.CreateNet(validation_model.net, overwrite=True)\n",
    "    \n",
    "    # Iterate over all validation dataset\n",
    "    all_accuracy = []\n",
    "    for i in range(X_valid.shape[0]//100):\n",
    "        workspace.RunNet(validation_model.net.Proto().name)\n",
    "        all_accuracy.append(workspace.FetchBlob('accuracy'))\n",
    "    \n",
    "    # Return mean accuracy for validation dataset\n",
    "    return np.array(all_accuracy).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #50/100 TIME_per_epoch: 0.126s TRAIN_Loss: 1.7594 TRAIN_Acc: 0.6100 VAL_Acc: 0.0857\n",
      "Epoch #100/100 TIME_per_epoch: 0.113s TRAIN_Loss: 1.6859 TRAIN_Acc: 0.7400 VAL_Acc: 0.0839\n"
     ]
    }
   ],
   "source": [
    "# Initialize out training model\n",
    "workspace.RunNetOnce(train_model.param_init_net)\n",
    "workspace.CreateNet(train_model.net, overwrite=True)\n",
    "\n",
    "# Iterate over all epochs\n",
    "# NUMBER_OF_EPOCHS = 10000\n",
    "NUMBER_OF_EPOCHS = 100\n",
    "for i in range(NUMBER_OF_EPOCHS):\n",
    "    # Train our model\n",
    "    start_time = time.time()\n",
    "    workspace.RunNet(train_model.net.Proto().name)\n",
    "    \n",
    "    # Once per 20 epochs let's run validation and print results\n",
    "    if (i+1) % 50 == 0:\n",
    "        train_loss = workspace.FetchBlob('loss')\n",
    "        train_accuracy = workspace.FetchBlob('accuracy')\n",
    "        val_accuracy = calculate_validation_accuracy()\n",
    "        epoch_time = time.time()-start_time\n",
    "        print(('Epoch #%d/%d TIME_per_epoch: %.3fs '+\n",
    "               'TRAIN_Loss: %.4f TRAIN_Acc: %.4f '+\n",
    "               'VAL_Acc: %.4f') % (i+1, NUMBER_OF_EPOCHS, epoch_time, train_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Relu.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Elu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting #20/280.0...\n",
      "Predicting #40/280.0...\n",
      "Predicting #60/280.0...\n",
      "Predicting #80/280.0...\n",
      "Predicting #100/280.0...\n",
      "Predicting #120/280.0...\n",
      "Predicting #140/280.0...\n",
      "Predicting #160/280.0...\n",
      "Predicting #180/280.0...\n",
      "Predicting #200/280.0...\n",
      "Predicting #220/280.0...\n",
      "Predicting #240/280.0...\n",
      "Predicting #260/280.0...\n",
      "Predicting #280/280.0...\n"
     ]
    }
   ],
   "source": [
    "# Initialize out prediction model\n",
    "test_model = create_model('test_model', 'db_test', train=False, accuracy=False)\n",
    "workspace.RunNetOnce(test_model.param_init_net)\n",
    "workspace.CreateNet(test_model.net, overwrite=True)\n",
    "\n",
    "# Iterate over all test dataset\n",
    "predicted_labels = []\n",
    "for i in range(X_test.shape[0]//100):\n",
    "    # Run our model for predicting labels\n",
    "    workspace.RunNet(test_model.net.Proto().name)\n",
    "    batch_prediction = workspace.FetchBlob('softmax')\n",
    "    if (i+1) % 20 == 0:\n",
    "        print('Predicting #{}/{}...'.format(i+1, X_test.shape[0]/100))\n",
    "    \n",
    "    # Retrieve labels\n",
    "    for prediction in batch_prediction:\n",
    "        predicted_labels.append(np.argmax(prediction))  # Label = index of max argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
