{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n",
      "WARNING:root:Debug message: No module named 'caffe2.python.caffe2_pybind11_state_hip'\n",
      "INFO:caffe2.python.net_drawer:Cannot import pydot, which is required for drawing a network. This can usually be installed in python with \"pip install pydot\". Also, pydot requires graphviz to convert dot files to pdf: in ubuntu, this can usually be installed with \"sudo apt-get install graphviz\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_drawer will not run correctly. Please install the correct dependencies.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython import display\n",
    "from caffe2.proto import caffe2_pb2\n",
    "from caffe2.python import cnn, core, utils, workspace, net_drawer\n",
    "\n",
    "train_csv = pd.read_csv('digit-recognizer/train.csv')\n",
    "test_csv = pd.read_csv('digit-recognizer/test.csv')\n",
    "db_type = \"minidb\" # \"leveldb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from CSV files\n",
    "X_train = train_csv.iloc[:,1:].values.astype('float32')\n",
    "y_train = train_csv.iloc[:,0].values.astype('int32')\n",
    "X_test = test_csv.values.astype('float32')\n",
    "\n",
    "# Reshape all images (1x784 -> 28x28)\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28)\n",
    "\n",
    "# Add feature layerab\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature standardization\n",
    "mean_px = X_train.mean().astype(np.float32)\n",
    "std_px = X_train.std().astype(np.float32)\n",
    "standardize = lambda image: (image - mean_px) / std_px\n",
    "\n",
    "# Preprocessing\n",
    "X_train = np.array([standardize(image) for image in X_train])\n",
    "X_test = np.array([standardize(image) for image in X_test])\n",
    "\n",
    "# Split all dataset for training and validation set\n",
    "X_validation = np.array(X_train[:1000])\n",
    "y_validation = np.array(y_train[:1000])\n",
    "X_train = np.array(X_train[1000:])\n",
    "y_train = np.array(y_train[1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(db_name, images, labels=None):\n",
    "    # Create empty leveldb database\n",
    "    db = core.C.create_db(db_type, db_name, core.C.Mode.new)\n",
    "    transaction = db.new_transaction()\n",
    "    \n",
    "    # Move all data to the database\n",
    "    for i in range(images.shape[0]):\n",
    "        tensor_protos = caffe2_pb2.TensorProtos()\n",
    "        \n",
    "        # Copy image with MNIST number\n",
    "        img_tensor = tensor_protos.protos.add()\n",
    "        img_tensor.dims.extend(images[i].shape)\n",
    "        img_tensor.data_type = 1\n",
    "        flatten_img = images[i].reshape(np.prod(images[i].shape))\n",
    "        img_tensor.float_data.extend(flatten_img)\n",
    "\n",
    "        # Copy label for each number\n",
    "        label_tensor = tensor_protos.protos.add()\n",
    "        label_tensor.data_type = 2\n",
    "        if labels is not None:\n",
    "            label_tensor.int32_data.append(labels[i])\n",
    "        else:\n",
    "            label_tensor.int32_data.append(-1)\n",
    "\n",
    "        # Add data in transaction\n",
    "        transaction.put('%0.6d' % i, tensor_protos.SerializeToString())\n",
    "\n",
    "    # Close the transaction and close the database\n",
    "    del transaction\n",
    "    del db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all databases\n",
    "create_database('/tmp/db_train', X_train, y_train)\n",
    "create_database('/tmp/db_validation', X_validation, y_validation)\n",
    "create_database('/tmp/db_test', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, db_name, batch_size=100, train=True, accuracy=True):\n",
    "    # Create empty model with CCN model helper (and initialize if needed for training)\n",
    "    if train:\n",
    "        model = cnn.CNNModelHelper(order=\"NCHW\", name=model_name)\n",
    "    else:\n",
    "        model = cnn.CNNModelHelper(order=\"NCHW\", name=model_name, init_params=False)\n",
    "\n",
    "    # Prepare data input operator that will fetch data from DB\n",
    "    data, label = model.TensorProtosDBInput([], ['data', 'label'], batch_size=batch_size, db=db_name, db_type=db_type)\n",
    "    data = model.StopGradient(data, data)\n",
    "    \n",
    "    # First convolution: 28 x 28 -> 24 x 24\n",
    "    conv1 = model.Conv(data, 'conv1', dim_in=1, dim_out=20, kernel=5)\n",
    "    \n",
    "    # First pooling: 24 x 24 -> 12 x 12\n",
    "    pool1 = model.MaxPool(conv1, 'pool1', kernel=2, stride=2)\n",
    "    \n",
    "    # Second convolution: 12 x 12 -> 8 x 8\n",
    "    conv2 = model.Conv(pool1, 'conv2', dim_in=20, dim_out=50, kernel=5)\n",
    "    \n",
    "    # Second pooling: 8 x 8 -> 4 x 4\n",
    "    pool2 = model.MaxPool(conv2, 'pool2', kernel=2, stride=2)\n",
    "    \n",
    "    # Fully connected layers at the end\n",
    "    fc3 = model.FC(pool2, 'fc3', dim_in=50 * 4 * 4, dim_out=500) # 50 * 4 * 4 = dim_out from previous layer * image size\n",
    "    fc3 = model.Relu(fc3, fc3)\n",
    "    pred = model.FC(fc3, 'pred', 500, 10)\n",
    "    softmax = model.Softmax(pred, 'softmax')\n",
    "    \n",
    "    # Check if we need to add training operators\n",
    "    if train:\n",
    "        # Prepare Cross Entropy operators with loss\n",
    "        xent = model.LabelCrossEntropy([softmax, label], 'xent')\n",
    "        loss = model.AveragedLoss(xent, \"loss\")\n",
    "\n",
    "        # Add all gradient operators that will be needed to calculate our loss and train our model\n",
    "        model.AddGradientOperators([loss])\n",
    "        \n",
    "        # Prepare variables for SGD\n",
    "        ITER = model.Iter(\"iter\")\n",
    "        LR = model.LearningRate(ITER, \"LR\", base_lr=-0.1, policy=\"step\", stepsize=1, gamma=0.999)\n",
    "        ONE = model.param_init_net.ConstantFill([], \"ONE\", shape=[1], value=1.0)\n",
    "        \n",
    "        # Update all gradients for each params\n",
    "        for param in model.params:\n",
    "            # Note how we get the gradient of each parameter - CNNModelHelper keeps\n",
    "            # track of that\n",
    "            param_grad = model.param_to_grad[param]\n",
    "            \n",
    "            # The update is a simple weighted sum: param = param + param_grad * LR\n",
    "            model.WeightedSum([param, ONE, param_grad, LR], param)\n",
    "    \n",
    "    # Add accuracy metrics if needed\n",
    "    if accuracy:\n",
    "        model.Accuracy([softmax, label], \"accuracy\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[====DEPRECATE WARNING====]: you are creating an object from CNNModelHelper class which will be deprecated soon. Please use ModelHelper object with brew module. For more information, please refer to caffe2.ai and python/brew.py, python/brew_test.py for more information.\n",
      "WARNING:root:[====DEPRECATE WARNING====]: you are creating an object from CNNModelHelper class which will be deprecated soon. Please use ModelHelper object with brew module. For more information, please refer to caffe2.ai and python/brew.py, python/brew_test.py for more information.\n",
      "WARNING:root:[====DEPRECATE WARNING====]: you are creating an object from CNNModelHelper class which will be deprecated soon. Please use ModelHelper object with brew module. For more information, please refer to caffe2.ai and python/brew.py, python/brew_test.py for more information.\n"
     ]
    }
   ],
   "source": [
    "# Create all needed models\n",
    "training_model = create_model('mnist_train', '/tmp/db_train')\n",
    "validation_model = create_model('mnist_validation', '/tmp/db_validation', train=False)\n",
    "test_model = create_model('mnist_test', '/tmp/db_test', train=False, accuracy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_validation_accuracy():\n",
    "    # Initialize our model\n",
    "    # workspace.RunNetOnce(validation_model.param_init_net)\n",
    "    workspace.CreateNet(validation_model.net, overwrite=True)\n",
    "    \n",
    "    # Iterate over all validation dataset\n",
    "    all_accuracy = []\n",
    "    for i in range(X_validation.shape[0]//100):\n",
    "        workspace.RunNet(validation_model.net.Proto().name)\n",
    "        all_accuracy.append(workspace.FetchBlob('accuracy'))\n",
    "    \n",
    "    # Return mean accuracy for validation dataset\n",
    "    return np.array(all_accuracy).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect():\n",
    "    b = workspace.FetchBlob(\"fc3_b\")\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20/1000 TIME_per_epoch: 0.304s TRAIN_Loss: 0.5574 TRAIN_Acc: 0.8100 VAL_Acc: 0.8510\n",
      "Epoch #40/1000 TIME_per_epoch: 0.427s TRAIN_Loss: 0.2662 TRAIN_Acc: 0.9200 VAL_Acc: 0.8800\n",
      "Epoch #60/1000 TIME_per_epoch: 0.303s TRAIN_Loss: 0.2863 TRAIN_Acc: 0.8800 VAL_Acc: 0.8960\n",
      "Epoch #80/1000 TIME_per_epoch: 0.319s TRAIN_Loss: 0.1433 TRAIN_Acc: 0.9600 VAL_Acc: 0.9430\n",
      "Epoch #100/1000 TIME_per_epoch: 0.287s TRAIN_Loss: 0.4032 TRAIN_Acc: 0.8400 VAL_Acc: 0.8730\n",
      "Epoch #120/1000 TIME_per_epoch: 0.284s TRAIN_Loss: 0.2020 TRAIN_Acc: 0.9500 VAL_Acc: 0.9420\n",
      "Epoch #140/1000 TIME_per_epoch: 0.283s TRAIN_Loss: 0.1079 TRAIN_Acc: 0.9700 VAL_Acc: 0.9680\n",
      "Epoch #160/1000 TIME_per_epoch: 0.287s TRAIN_Loss: 0.1077 TRAIN_Acc: 0.9600 VAL_Acc: 0.9710\n",
      "Epoch #180/1000 TIME_per_epoch: 0.285s TRAIN_Loss: 0.0874 TRAIN_Acc: 0.9800 VAL_Acc: 0.9730\n",
      "Epoch #200/1000 TIME_per_epoch: 0.308s TRAIN_Loss: 0.1993 TRAIN_Acc: 0.9400 VAL_Acc: 0.9660\n",
      "Epoch #220/1000 TIME_per_epoch: 0.361s TRAIN_Loss: 0.0828 TRAIN_Acc: 0.9600 VAL_Acc: 0.9700\n",
      "Epoch #240/1000 TIME_per_epoch: 0.292s TRAIN_Loss: 0.0661 TRAIN_Acc: 0.9800 VAL_Acc: 0.9760\n",
      "Epoch #260/1000 TIME_per_epoch: 0.288s TRAIN_Loss: 0.0709 TRAIN_Acc: 0.9800 VAL_Acc: 0.9770\n",
      "Epoch #280/1000 TIME_per_epoch: 0.347s TRAIN_Loss: 0.0751 TRAIN_Acc: 0.9900 VAL_Acc: 0.9760\n",
      "Epoch #300/1000 TIME_per_epoch: 0.319s TRAIN_Loss: 0.0600 TRAIN_Acc: 0.9800 VAL_Acc: 0.9780\n",
      "Epoch #320/1000 TIME_per_epoch: 0.294s TRAIN_Loss: 0.0663 TRAIN_Acc: 0.9800 VAL_Acc: 0.9800\n",
      "Epoch #340/1000 TIME_per_epoch: 0.306s TRAIN_Loss: 0.0656 TRAIN_Acc: 0.9700 VAL_Acc: 0.9780\n",
      "Epoch #360/1000 TIME_per_epoch: 0.297s TRAIN_Loss: 0.1187 TRAIN_Acc: 0.9600 VAL_Acc: 0.9790\n",
      "Epoch #380/1000 TIME_per_epoch: 0.429s TRAIN_Loss: 0.0595 TRAIN_Acc: 0.9700 VAL_Acc: 0.9810\n",
      "Epoch #400/1000 TIME_per_epoch: 0.368s TRAIN_Loss: 0.0627 TRAIN_Acc: 0.9900 VAL_Acc: 0.9830\n",
      "Epoch #420/1000 TIME_per_epoch: 0.278s TRAIN_Loss: 0.0826 TRAIN_Acc: 0.9700 VAL_Acc: 0.9810\n",
      "Epoch #440/1000 TIME_per_epoch: 0.283s TRAIN_Loss: 0.0626 TRAIN_Acc: 0.9800 VAL_Acc: 0.9780\n",
      "Epoch #460/1000 TIME_per_epoch: 0.279s TRAIN_Loss: 0.0293 TRAIN_Acc: 0.9900 VAL_Acc: 0.9830\n",
      "Epoch #480/1000 TIME_per_epoch: 0.289s TRAIN_Loss: 0.0560 TRAIN_Acc: 0.9800 VAL_Acc: 0.9790\n",
      "Epoch #500/1000 TIME_per_epoch: 0.278s TRAIN_Loss: 0.0594 TRAIN_Acc: 0.9900 VAL_Acc: 0.9830\n",
      "Epoch #520/1000 TIME_per_epoch: 0.290s TRAIN_Loss: 0.0569 TRAIN_Acc: 0.9800 VAL_Acc: 0.9810\n",
      "Epoch #540/1000 TIME_per_epoch: 0.285s TRAIN_Loss: 0.0633 TRAIN_Acc: 0.9700 VAL_Acc: 0.9870\n",
      "Epoch #560/1000 TIME_per_epoch: 0.277s TRAIN_Loss: 0.0626 TRAIN_Acc: 0.9800 VAL_Acc: 0.9860\n",
      "Epoch #580/1000 TIME_per_epoch: 0.284s TRAIN_Loss: 0.0755 TRAIN_Acc: 0.9900 VAL_Acc: 0.9840\n",
      "Epoch #600/1000 TIME_per_epoch: 0.292s TRAIN_Loss: 0.0438 TRAIN_Acc: 0.9800 VAL_Acc: 0.9860\n",
      "Epoch #620/1000 TIME_per_epoch: 0.289s TRAIN_Loss: 0.0159 TRAIN_Acc: 1.0000 VAL_Acc: 0.9900\n",
      "Epoch #640/1000 TIME_per_epoch: 0.296s TRAIN_Loss: 0.0371 TRAIN_Acc: 0.9800 VAL_Acc: 0.9830\n",
      "Epoch #660/1000 TIME_per_epoch: 0.277s TRAIN_Loss: 0.0763 TRAIN_Acc: 0.9800 VAL_Acc: 0.9850\n",
      "Epoch #680/1000 TIME_per_epoch: 0.283s TRAIN_Loss: 0.0184 TRAIN_Acc: 1.0000 VAL_Acc: 0.9850\n",
      "Epoch #700/1000 TIME_per_epoch: 0.282s TRAIN_Loss: 0.0601 TRAIN_Acc: 0.9700 VAL_Acc: 0.9870\n",
      "Epoch #720/1000 TIME_per_epoch: 0.311s TRAIN_Loss: 0.0539 TRAIN_Acc: 0.9800 VAL_Acc: 0.9840\n",
      "Epoch #740/1000 TIME_per_epoch: 0.304s TRAIN_Loss: 0.0187 TRAIN_Acc: 0.9900 VAL_Acc: 0.9860\n",
      "Epoch #760/1000 TIME_per_epoch: 0.307s TRAIN_Loss: 0.0119 TRAIN_Acc: 1.0000 VAL_Acc: 0.9830\n",
      "Epoch #780/1000 TIME_per_epoch: 0.284s TRAIN_Loss: 0.0100 TRAIN_Acc: 1.0000 VAL_Acc: 0.9790\n",
      "Epoch #800/1000 TIME_per_epoch: 0.432s TRAIN_Loss: 0.0393 TRAIN_Acc: 0.9900 VAL_Acc: 0.9800\n",
      "Epoch #820/1000 TIME_per_epoch: 0.288s TRAIN_Loss: 0.0325 TRAIN_Acc: 0.9900 VAL_Acc: 0.9870\n",
      "Epoch #840/1000 TIME_per_epoch: 0.286s TRAIN_Loss: 0.1266 TRAIN_Acc: 0.9600 VAL_Acc: 0.9830\n",
      "Epoch #860/1000 TIME_per_epoch: 0.282s TRAIN_Loss: 0.0522 TRAIN_Acc: 0.9800 VAL_Acc: 0.9860\n",
      "Epoch #880/1000 TIME_per_epoch: 0.283s TRAIN_Loss: 0.0317 TRAIN_Acc: 0.9800 VAL_Acc: 0.9870\n",
      "Epoch #900/1000 TIME_per_epoch: 0.280s TRAIN_Loss: 0.0164 TRAIN_Acc: 1.0000 VAL_Acc: 0.9830\n",
      "Epoch #920/1000 TIME_per_epoch: 0.282s TRAIN_Loss: 0.0183 TRAIN_Acc: 1.0000 VAL_Acc: 0.9880\n",
      "Epoch #940/1000 TIME_per_epoch: 0.290s TRAIN_Loss: 0.0477 TRAIN_Acc: 0.9900 VAL_Acc: 0.9840\n",
      "Epoch #960/1000 TIME_per_epoch: 0.289s TRAIN_Loss: 0.0221 TRAIN_Acc: 1.0000 VAL_Acc: 0.9850\n",
      "Epoch #980/1000 TIME_per_epoch: 0.314s TRAIN_Loss: 0.0100 TRAIN_Acc: 1.0000 VAL_Acc: 0.9880\n",
      "Epoch #1000/1000 TIME_per_epoch: 0.322s TRAIN_Loss: 0.0241 TRAIN_Acc: 0.9900 VAL_Acc: 0.9860\n"
     ]
    }
   ],
   "source": [
    "# Initialize out training model\n",
    "workspace.RunNetOnce(validation_model.param_init_net)\n",
    "workspace.RunNetOnce(test_model.param_init_net)\n",
    "workspace.RunNetOnce(training_model.param_init_net)\n",
    "workspace.CreateNet(training_model.net, overwrite=True)\n",
    "\n",
    "# inspect()\n",
    "# Iterate over all epochs\n",
    "NUMBER_OF_EPOCHS = 1000\n",
    "for i in range(NUMBER_OF_EPOCHS):\n",
    "    # Train our model\n",
    "    start_time = time.time()\n",
    "    workspace.RunNet(training_model.net.Proto().name)\n",
    "    \n",
    "    # Once per 20 epochs let's run validation and print results\n",
    "    if (i+1) % 20 == 0:\n",
    "        train_loss = workspace.FetchBlob('loss')\n",
    "        train_accuracy = workspace.FetchBlob('accuracy')\n",
    "        val_accuracy = calculate_validation_accuracy()\n",
    "        epoch_time = time.time()-start_time\n",
    "        print(('Epoch #%d/%d TIME_per_epoch: %.3fs '+\n",
    "               'TRAIN_Loss: %.4f TRAIN_Acc: %.4f '+\n",
    "               'VAL_Acc: %.4f') % (i+1, NUMBER_OF_EPOCHS, epoch_time, train_loss, train_accuracy, val_accuracy))\n",
    "        # inspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting #20/280.0...\n",
      "Predicting #40/280.0...\n",
      "Predicting #60/280.0...\n",
      "Predicting #80/280.0...\n",
      "Predicting #100/280.0...\n",
      "Predicting #120/280.0...\n",
      "Predicting #140/280.0...\n",
      "Predicting #160/280.0...\n",
      "Predicting #180/280.0...\n",
      "Predicting #200/280.0...\n",
      "Predicting #220/280.0...\n",
      "Predicting #240/280.0...\n",
      "Predicting #260/280.0...\n",
      "Predicting #280/280.0...\n"
     ]
    }
   ],
   "source": [
    "# Initialize out prediction model\n",
    "# workspace.RunNetOnce(test_model.param_init_net)\n",
    "workspace.CreateNet(test_model.net, overwrite=True)\n",
    "\n",
    "# Iterate over all test dataset\n",
    "predicted_labels = []\n",
    "for i in range(X_test.shape[0]//100):\n",
    "    # Run our model for predicting labels\n",
    "    workspace.RunNet(test_model.net.Proto().name)\n",
    "    batch_prediction = workspace.FetchBlob('softmax')\n",
    "    if (i+1) % 20 == 0:\n",
    "        print('Predicting #{}/{}...'.format(i+1, X_test.shape[0]/100))\n",
    "    \n",
    "    # Retrieve labels\n",
    "    for prediction in batch_prediction:\n",
    "        predicted_labels.append(np.argmax(prediction))  # Label = index of max argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dict = {}\n",
    "for i, v in enumerate(predicted_labels):\n",
    "    raw_dict[i + 1] = v\n",
    "\n",
    "out_df = pd.DataFrame(\n",
    "    data={\n",
    "        \"ImageId\": raw_dict.keys(),\n",
    "        \"Label\": raw_dict.values(),\n",
    "    }\n",
    ")\n",
    "out_df.to_csv('/tmp/caffe2_cnn', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
