{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n",
      "WARNING:root:Debug message: No module named 'caffe2.python.caffe2_pybind11_state_hip'\n",
      "INFO:caffe2.python.net_drawer:Cannot import pydot, which is required for drawing a network. This can usually be installed in python with \"pip install pydot\". Also, pydot requires graphviz to convert dot files to pdf: in ubuntu, this can usually be installed with \"sudo apt-get install graphviz\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_drawer will not run correctly. Please install the correct dependencies.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython import display\n",
    "from caffe2.proto import caffe2_pb2\n",
    "from caffe2.python import cnn, core, utils, workspace, net_drawer\n",
    "\n",
    "train_csv = pd.read_csv('digit-recognizer/train.csv')\n",
    "test_csv = pd.read_csv('digit-recognizer/test.csv')\n",
    "db_type = \"minidb\" # \"leveldb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from CSV files\n",
    "X_train = train_csv.iloc[:,1:].values.astype('float32')\n",
    "y_train = train_csv.iloc[:,0].values.astype('int32')\n",
    "X_test = test_csv.values.astype('float32')\n",
    "\n",
    "# Reshape all images (1x784 -> 28x28)\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28)\n",
    "\n",
    "# Add feature layerab\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature standardization\n",
    "mean_px = X_train.mean().astype(np.float32)\n",
    "std_px = X_train.std().astype(np.float32)\n",
    "standardize = lambda image: (image - mean_px) / std_px\n",
    "\n",
    "# Preprocessing\n",
    "X_train = np.array([standardize(image) for image in X_train])\n",
    "X_test = np.array([standardize(image) for image in X_test])\n",
    "\n",
    "# Split all dataset for training and validation set\n",
    "X_validation = np.array(X_train[:1000])\n",
    "y_validation = np.array(y_train[:1000])\n",
    "X_train = np.array(X_train[1000:])\n",
    "y_train = np.array(y_train[1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(db_name, images, labels=None):\n",
    "    # Create empty leveldb database\n",
    "    db = core.C.create_db(db_type, db_name, core.C.Mode.write)\n",
    "    transaction = db.new_transaction()\n",
    "    \n",
    "    # Move all data to the database\n",
    "    for i in range(images.shape[0]):\n",
    "        tensor_protos = caffe2_pb2.TensorProtos()\n",
    "        \n",
    "        # Copy image with MNIST number\n",
    "        img_tensor = tensor_protos.protos.add()\n",
    "        img_tensor.dims.extend(images[i].shape)\n",
    "        img_tensor.data_type = 1\n",
    "        flatten_img = images[i].reshape(np.prod(images[i].shape))\n",
    "        img_tensor.float_data.extend(flatten_img)\n",
    "\n",
    "        # Copy label for each number\n",
    "        label_tensor = tensor_protos.protos.add()\n",
    "        label_tensor.data_type = 2\n",
    "        if labels is not None:\n",
    "            label_tensor.int32_data.append(labels[i])\n",
    "        else:\n",
    "            label_tensor.int32_data.append(-1)\n",
    "\n",
    "        # Add data in transaction\n",
    "        transaction.put('%0.6d' % i, tensor_protos.SerializeToString())\n",
    "\n",
    "    # Close the transaction and close the database\n",
    "    del transaction\n",
    "    del db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all databases\n",
    "create_database('db_train', X_train, y_train)\n",
    "create_database('db_validation', X_validation, y_validation)\n",
    "create_database('db_test', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, db_name, batch_size=100, train=True, accuracy=True):\n",
    "    # Create empty model with CCN model helper (and initialize if needed for training)\n",
    "    if train:\n",
    "        model = cnn.CNNModelHelper(order=\"NCHW\", name=model_name)\n",
    "    else:\n",
    "        model = cnn.CNNModelHelper(order=\"NCHW\", name=model_name, init_params=False)\n",
    "\n",
    "    # Prepare data input operator that will fetch data from DB\n",
    "    data, label = model.TensorProtosDBInput([], ['data', 'label'], batch_size=batch_size, db=db_name, db_type=db_type)\n",
    "    data = model.StopGradient(data, data)\n",
    "    \n",
    "    # First convolution: 28 x 28 -> 24 x 24\n",
    "    conv1 = model.Conv(data, 'conv1', dim_in=1, dim_out=20, kernel=5)\n",
    "    \n",
    "    # First pooling: 24 x 24 -> 12 x 12\n",
    "    pool1 = model.MaxPool(conv1, 'pool1', kernel=2, stride=2)\n",
    "    \n",
    "    # Second convolution: 12 x 12 -> 8 x 8\n",
    "    conv2 = model.Conv(pool1, 'conv2', dim_in=20, dim_out=50, kernel=5)\n",
    "    \n",
    "    # Second pooling: 8 x 8 -> 4 x 4\n",
    "    pool2 = model.MaxPool(conv2, 'pool2', kernel=2, stride=2)\n",
    "    \n",
    "    # Fully connected layers at the end\n",
    "    fc3 = model.FC(pool2, 'fc3', dim_in=50 * 4 * 4, dim_out=500) # 50 * 4 * 4 = dim_out from previous layer * image size\n",
    "    fc3 = model.Relu(fc3, fc3)\n",
    "    pred = model.FC(fc3, 'pred', 500, 10)\n",
    "    softmax = model.Softmax(pred, 'softmax')\n",
    "    \n",
    "    # Check if we need to add training operators\n",
    "    if train:\n",
    "        # Prepare Cross Entropy operators with loss\n",
    "        xent = model.LabelCrossEntropy([softmax, label], 'xent')\n",
    "        loss = model.AveragedLoss(xent, \"loss\")\n",
    "\n",
    "        # Add all gradient operators that will be needed to calculate our loss and train our model\n",
    "        model.AddGradientOperators([loss])\n",
    "        \n",
    "        # Prepare variables for SGD\n",
    "        ITER = model.Iter(\"iter\")\n",
    "        LR = model.LearningRate(ITER, \"LR\", base_lr=-0.1, policy=\"step\", stepsize=1, gamma=0.999)\n",
    "        ONE = model.param_init_net.ConstantFill([], \"ONE\", shape=[1], value=1.0)\n",
    "        \n",
    "        # Update all gradients for each params\n",
    "        for param in model.params:\n",
    "            # Note how we get the gradient of each parameter - CNNModelHelper keeps\n",
    "            # track of that\n",
    "            param_grad = model.param_to_grad[param]\n",
    "            \n",
    "            # The update is a simple weighted sum: param = param + param_grad * LR\n",
    "            model.WeightedSum([param, ONE, param_grad, LR], param)\n",
    "    \n",
    "    # Add accuracy metrics if needed\n",
    "    if accuracy:\n",
    "        model.Accuracy([softmax, label], \"accuracy\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[====DEPRECATE WARNING====]: you are creating an object from CNNModelHelper class which will be deprecated soon. Please use ModelHelper object with brew module. For more information, please refer to caffe2.ai and python/brew.py, python/brew_test.py for more information.\n",
      "WARNING:root:[====DEPRECATE WARNING====]: you are creating an object from CNNModelHelper class which will be deprecated soon. Please use ModelHelper object with brew module. For more information, please refer to caffe2.ai and python/brew.py, python/brew_test.py for more information.\n",
      "WARNING:root:[====DEPRECATE WARNING====]: you are creating an object from CNNModelHelper class which will be deprecated soon. Please use ModelHelper object with brew module. For more information, please refer to caffe2.ai and python/brew.py, python/brew_test.py for more information.\n"
     ]
    }
   ],
   "source": [
    "# Create all needed models\n",
    "training_model = create_model('mnist_train', 'db_train')\n",
    "validation_model = create_model('mnist_validation', 'db_validation', train=False)\n",
    "test_model = create_model('mnist_test', 'db_test', train=False, accuracy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_validation_accuracy():\n",
    "    # Initialize our model\n",
    "    workspace.RunNetOnce(validation_model.param_init_net)\n",
    "    workspace.CreateNet(validation_model.net, overwrite=True)\n",
    "    \n",
    "    # Iterate over all validation dataset\n",
    "    all_accuracy = []\n",
    "    for i in range(X_validation.shape[0]//100):\n",
    "        workspace.RunNet(validation_model.net.Proto().name)\n",
    "        all_accuracy.append(workspace.FetchBlob('accuracy'))\n",
    "    \n",
    "    # Return mean accuracy for validation dataset\n",
    "    return np.array(all_accuracy).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20/1000 TIME_per_epoch: 0.271s TRAIN_Loss: 42.8281 TRAIN_Acc: 0.0700 VAL_Acc: 0.0890\n",
      "Epoch #40/1000 TIME_per_epoch: 0.415s TRAIN_Loss: 39.6045 TRAIN_Acc: 0.1400 VAL_Acc: 0.1050\n",
      "Epoch #60/1000 TIME_per_epoch: 0.427s TRAIN_Loss: 38.6834 TRAIN_Acc: 0.1600 VAL_Acc: 0.1050\n",
      "Epoch #80/1000 TIME_per_epoch: 0.278s TRAIN_Loss: 42.8281 TRAIN_Acc: 0.0700 VAL_Acc: 0.1050\n",
      "Epoch #100/1000 TIME_per_epoch: 0.281s TRAIN_Loss: 43.7491 TRAIN_Acc: 0.0500 VAL_Acc: 0.1050\n",
      "Epoch #120/1000 TIME_per_epoch: 0.291s TRAIN_Loss: 40.0650 TRAIN_Acc: 0.1300 VAL_Acc: 0.1050\n",
      "Epoch #140/1000 TIME_per_epoch: 0.284s TRAIN_Loss: 41.4465 TRAIN_Acc: 0.1000 VAL_Acc: 0.1050\n",
      "Epoch #160/1000 TIME_per_epoch: 0.287s TRAIN_Loss: 43.7491 TRAIN_Acc: 0.0500 VAL_Acc: 0.1050\n",
      "Epoch #180/1000 TIME_per_epoch: 0.286s TRAIN_Loss: 39.6045 TRAIN_Acc: 0.1400 VAL_Acc: 0.1050\n",
      "Epoch #200/1000 TIME_per_epoch: 0.288s TRAIN_Loss: 41.9071 TRAIN_Acc: 0.0900 VAL_Acc: 0.1050\n",
      "Epoch #220/1000 TIME_per_epoch: 0.281s TRAIN_Loss: 41.4465 TRAIN_Acc: 0.1000 VAL_Acc: 0.1050\n",
      "Epoch #240/1000 TIME_per_epoch: 0.289s TRAIN_Loss: 39.1439 TRAIN_Acc: 0.1500 VAL_Acc: 0.1050\n",
      "Epoch #260/1000 TIME_per_epoch: 0.278s TRAIN_Loss: 42.6180 TRAIN_Acc: 0.0700 VAL_Acc: 0.1040\n",
      "Epoch #280/1000 TIME_per_epoch: 0.279s TRAIN_Loss: 43.2886 TRAIN_Acc: 0.0600 VAL_Acc: 0.1040\n",
      "Epoch #300/1000 TIME_per_epoch: 0.275s TRAIN_Loss: 40.6471 TRAIN_Acc: 0.1100 VAL_Acc: 0.1120\n",
      "Epoch #320/1000 TIME_per_epoch: 0.280s TRAIN_Loss: 39.6045 TRAIN_Acc: 0.1400 VAL_Acc: 0.0960\n",
      "Epoch #340/1000 TIME_per_epoch: 0.275s TRAIN_Loss: 40.0650 TRAIN_Acc: 0.1300 VAL_Acc: 0.1050\n",
      "Epoch #360/1000 TIME_per_epoch: 0.288s TRAIN_Loss: 42.8281 TRAIN_Acc: 0.0700 VAL_Acc: 0.1050\n",
      "Epoch #380/1000 TIME_per_epoch: 0.302s TRAIN_Loss: 43.2886 TRAIN_Acc: 0.0600 VAL_Acc: 0.1050\n",
      "Epoch #400/1000 TIME_per_epoch: 0.276s TRAIN_Loss: 43.2886 TRAIN_Acc: 0.0600 VAL_Acc: 0.1050\n",
      "Epoch #420/1000 TIME_per_epoch: 0.278s TRAIN_Loss: 41.4465 TRAIN_Acc: 0.1000 VAL_Acc: 0.1050\n",
      "Epoch #440/1000 TIME_per_epoch: 0.355s TRAIN_Loss: 42.8281 TRAIN_Acc: 0.0700 VAL_Acc: 0.1050\n",
      "Epoch #460/1000 TIME_per_epoch: 0.346s TRAIN_Loss: 38.2229 TRAIN_Acc: 0.1700 VAL_Acc: 0.1050\n",
      "Epoch #480/1000 TIME_per_epoch: 0.280s TRAIN_Loss: 42.8281 TRAIN_Acc: 0.0700 VAL_Acc: 0.1050\n",
      "Epoch #500/1000 TIME_per_epoch: 0.277s TRAIN_Loss: 39.6045 TRAIN_Acc: 0.1400 VAL_Acc: 0.1050\n",
      "Epoch #520/1000 TIME_per_epoch: 0.328s TRAIN_Loss: 40.9860 TRAIN_Acc: 0.1100 VAL_Acc: 0.1050\n",
      "Epoch #540/1000 TIME_per_epoch: 0.271s TRAIN_Loss: 42.3676 TRAIN_Acc: 0.0800 VAL_Acc: 0.1050\n",
      "Epoch #560/1000 TIME_per_epoch: 0.321s TRAIN_Loss: 42.3676 TRAIN_Acc: 0.0800 VAL_Acc: 0.1050\n",
      "Epoch #580/1000 TIME_per_epoch: 0.308s TRAIN_Loss: 40.0650 TRAIN_Acc: 0.1300 VAL_Acc: 0.1050\n",
      "Epoch #600/1000 TIME_per_epoch: 0.282s TRAIN_Loss: 38.2229 TRAIN_Acc: 0.1700 VAL_Acc: 0.1050\n",
      "Epoch #620/1000 TIME_per_epoch: 0.308s TRAIN_Loss: 40.0650 TRAIN_Acc: 0.1300 VAL_Acc: 0.1050\n",
      "Epoch #640/1000 TIME_per_epoch: 0.439s TRAIN_Loss: 41.9071 TRAIN_Acc: 0.0900 VAL_Acc: 0.1050\n",
      "Epoch #660/1000 TIME_per_epoch: 0.284s TRAIN_Loss: 41.9071 TRAIN_Acc: 0.0900 VAL_Acc: 0.1050\n",
      "Epoch #680/1000 TIME_per_epoch: 0.314s TRAIN_Loss: 42.8281 TRAIN_Acc: 0.0700 VAL_Acc: 0.1050\n",
      "Epoch #700/1000 TIME_per_epoch: 0.273s TRAIN_Loss: 41.4465 TRAIN_Acc: 0.1000 VAL_Acc: 0.1050\n",
      "Epoch #720/1000 TIME_per_epoch: 0.272s TRAIN_Loss: 39.6045 TRAIN_Acc: 0.1400 VAL_Acc: 0.1050\n",
      "Epoch #740/1000 TIME_per_epoch: 0.279s TRAIN_Loss: 38.6834 TRAIN_Acc: 0.1600 VAL_Acc: 0.1050\n",
      "Epoch #760/1000 TIME_per_epoch: 0.275s TRAIN_Loss: 41.4465 TRAIN_Acc: 0.1000 VAL_Acc: 0.1050\n",
      "Epoch #780/1000 TIME_per_epoch: 0.283s TRAIN_Loss: 41.9071 TRAIN_Acc: 0.0900 VAL_Acc: 0.1050\n",
      "Epoch #800/1000 TIME_per_epoch: 0.272s TRAIN_Loss: 39.1439 TRAIN_Acc: 0.1500 VAL_Acc: 0.1050\n",
      "Epoch #820/1000 TIME_per_epoch: 0.283s TRAIN_Loss: 41.9071 TRAIN_Acc: 0.0900 VAL_Acc: 0.1050\n",
      "Epoch #840/1000 TIME_per_epoch: 0.438s TRAIN_Loss: 39.6045 TRAIN_Acc: 0.1400 VAL_Acc: 0.1050\n",
      "Epoch #860/1000 TIME_per_epoch: 0.273s TRAIN_Loss: 39.6045 TRAIN_Acc: 0.1400 VAL_Acc: 0.1050\n",
      "Epoch #880/1000 TIME_per_epoch: 0.292s TRAIN_Loss: 38.6834 TRAIN_Acc: 0.1600 VAL_Acc: 0.1050\n",
      "Epoch #900/1000 TIME_per_epoch: 0.278s TRAIN_Loss: 42.8281 TRAIN_Acc: 0.0700 VAL_Acc: 0.1050\n",
      "Epoch #920/1000 TIME_per_epoch: 0.278s TRAIN_Loss: 43.7491 TRAIN_Acc: 0.0500 VAL_Acc: 0.1050\n",
      "Epoch #940/1000 TIME_per_epoch: 0.275s TRAIN_Loss: 40.0650 TRAIN_Acc: 0.1300 VAL_Acc: 0.1050\n",
      "Epoch #960/1000 TIME_per_epoch: 0.276s TRAIN_Loss: 41.4465 TRAIN_Acc: 0.1000 VAL_Acc: 0.1050\n",
      "Epoch #980/1000 TIME_per_epoch: 0.401s TRAIN_Loss: 43.7491 TRAIN_Acc: 0.0500 VAL_Acc: 0.1050\n",
      "Epoch #1000/1000 TIME_per_epoch: 0.278s TRAIN_Loss: 39.6045 TRAIN_Acc: 0.1400 VAL_Acc: 0.1050\n"
     ]
    }
   ],
   "source": [
    "# Initialize out training model\n",
    "workspace.RunNetOnce(training_model.param_init_net)\n",
    "workspace.CreateNet(training_model.net, overwrite=True)\n",
    "\n",
    "# Iterate over all epochs\n",
    "NUMBER_OF_EPOCHS = 1000\n",
    "for i in range(NUMBER_OF_EPOCHS):\n",
    "    # Train our model\n",
    "    start_time = time.time()\n",
    "    workspace.RunNet(training_model.net.Proto().name)\n",
    "    \n",
    "    # Once per 20 epochs let's run validation and print results\n",
    "    if (i+1) % 20 == 0:\n",
    "        train_loss = workspace.FetchBlob('loss')\n",
    "        train_accuracy = workspace.FetchBlob('accuracy')\n",
    "        val_accuracy = calculate_validation_accuracy()\n",
    "        epoch_time = time.time()-start_time\n",
    "        print(('Epoch #%d/%d TIME_per_epoch: %.3fs '+\n",
    "               'TRAIN_Loss: %.4f TRAIN_Acc: %.4f '+\n",
    "               'VAL_Acc: %.4f') % (i+1, NUMBER_OF_EPOCHS, epoch_time, train_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize out prediction model\n",
    "workspace.RunNetOnce(test_model.param_init_net)\n",
    "workspace.CreateNet(test_model.net)\n",
    "\n",
    "# Iterate over all test dataset\n",
    "predicted_labels = []\n",
    "for i in range(X_test.shape[0]/100):\n",
    "    # Run our model for predicting labels\n",
    "    workspace.RunNet(test_model.net.Proto().name)\n",
    "    batch_prediction = workspace.FetchBlob('softmax')\n",
    "    if (i+1) % 20 == 0:\n",
    "        print('Predicting #{}/{}...'.format(i+1, X_test.shape[0]/100))\n",
    "    \n",
    "    # Retrieve labels\n",
    "    for prediction in batch_prediction:\n",
    "        predicted_labels.append(np.argmax(prediction))  # Label = index of max argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
